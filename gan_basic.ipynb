{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 10:42:29.212303: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 28\n",
    "cols = 28\n",
    "channels = 1\n",
    "image_size = (rows,cols,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an input noise or latent vector generate a fake image\n",
    "\n",
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)       \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(np.prod(image_size), activation='tanh'))\n",
    "    model.add(Reshape(image_size))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    generated_img = model(noise)    #Generated image\n",
    "\n",
    "    return Model(noise, generated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a fake image tell me the likelihood if it is real or fake\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=image_size))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=image_size)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    # print 5x5=25 generated images\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Steps:\n",
    "Train the discriminator:\n",
    "        we use gen and disc separately:\n",
    "                generate fake half batch size images from generator\n",
    "                ask discriminator for loss on these half real and half fake\n",
    "                average the loss\n",
    "Train the generator:\n",
    "        we use both the networks together, combined:\n",
    "                generate fake full batch size images from generator\n",
    "                pass it to the network saying the discriminator that these fake images are real (this is the way we hold disc constant while training gen)\n",
    "\"\"\"\n",
    "\n",
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=3) # make from 28x28 to 28x28x1\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #  Train Discriminator\n",
    "\n",
    "        # Select a random half batch of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # noise for the generator\n",
    "        # half_batch 1: rnd,rnd,rnd ... 100\n",
    "        # half batch 2: rnd,rnd,rnd ... 100 and so on\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "        # Generate a half batch of fake images by the generator first\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # ask discriminator for loss test on both real and fake images\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        \n",
    "        # average the loss\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
    "\n",
    "        #  Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "\n",
    "        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "        \n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('machine-learning-tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31384ce3dd6bc2c85be3c3d887d32a02f47760f0703449b13d0f58d482b7108e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
